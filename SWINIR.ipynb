{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a8e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/wschenst06g/Ming/MMSE_global/dodo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7685a3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KAIR'...\n",
      "remote: Enumerating objects: 1599, done.\u001b[K\n",
      "remote: Counting objects: 100% (667/667), done.\u001b[K\n",
      "remote: Compressing objects: 100% (373/373), done.\u001b[K\n",
      "remote: Total 1599 (delta 349), reused 582 (delta 287), pack-reused 932\u001b[K\n",
      "Receiving objects: 100% (1599/1599), 18.70 MiB | 13.19 MiB/s, done.\n",
      "Resolving deltas: 100% (907/907), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cszn/KAIR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1e1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAIR\t\t\t      efficient_model.pkl\r\n",
      "SWINIR.ipynb\t\t      hw4.py\r\n",
      "VRDL_HW2_RELEASED_DATASET     rsna-pneumonia\r\n",
      "VRDL_HW3\t\t      stage_2_train_unique.csv\r\n",
      "VRDL_HW4\t\t      yolov4\r\n",
      "covid19_classification.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a9a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wschenst06g/Ming/MMSE_global/dodo/KAIR\n"
     ]
    }
   ],
   "source": [
    "%cd KAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8afcbbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/wschenst06g/.local/lib/python3.8/site-packages (from -r requirement.txt (line 1)) (4.5.4.58)\n",
      "Requirement already satisfied: scikit-image in /home/wschenst06g/.local/lib/python3.8/site-packages (from -r requirement.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: pillow in /home/wschenst06g/.local/lib/python3.8/site-packages (from -r requirement.txt (line 3)) (8.4.0)\n",
      "Requirement already satisfied: torchvision in /home/wschenst06g/.local/lib/python3.8/site-packages (from -r requirement.txt (line 4)) (0.11.2)\n",
      "Requirement already satisfied: hdf5storage in /home/wschenst06g/.local/lib/python3.8/site-packages (from -r requirement.txt (line 5)) (0.1.18)\n",
      "Requirement already satisfied: ninja in /home/wschenst06g/.local/lib/python3.8/site-packages (from -r requirement.txt (line 6)) (1.10.2.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/wschenst06g/.local/lib/python3.8/site-packages (from opencv-python->-r requirement.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (from scikit-image->-r requirement.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: networkx>=1.8 in /home/wschenst06g/.local/lib/python3.8/site-packages (from scikit-image->-r requirement.txt (line 2)) (2.6.3)\n",
      "Requirement already satisfied: dask[array]>=0.9.0 in /home/wschenst06g/.local/lib/python3.8/site-packages (from scikit-image->-r requirement.txt (line 2)) (2021.12.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/wschenst06g/.local/lib/python3.8/site-packages (from scikit-image->-r requirement.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/wschenst06g/.local/lib/python3.8/site-packages (from scikit-image->-r requirement.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /home/wschenst06g/.local/lib/python3.8/site-packages (from scikit-image->-r requirement.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: torch==1.10.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (from torchvision->-r requirement.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.1->torchvision->-r requirement.txt (line 4)) (3.10.0.0)\n",
      "Requirement already satisfied: h5py>=2.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (from hdf5storage->-r requirement.txt (line 5)) (2.10.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/wschenst06g/.local/lib/python3.8/site-packages (from dask[array]>=0.9.0->scikit-image->-r requirement.txt (line 2)) (2021.11.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from dask[array]>=0.9.0->scikit-image->-r requirement.txt (line 2)) (21.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /home/wschenst06g/.local/lib/python3.8/site-packages (from dask[array]>=0.9.0->scikit-image->-r requirement.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /home/wschenst06g/.local/lib/python3.8/site-packages (from dask[array]>=0.9.0->scikit-image->-r requirement.txt (line 2)) (0.11.2)\n",
      "Requirement already satisfied: pyyaml in /home/wschenst06g/.local/lib/python3.8/site-packages (from dask[array]>=0.9.0->scikit-image->-r requirement.txt (line 2)) (5.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.0.0->scikit-image->-r requirement.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/wschenst06g/.local/lib/python3.8/site-packages (from matplotlib>=2.0.0->scikit-image->-r requirement.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (from matplotlib>=2.0.0->scikit-image->-r requirement.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.0.0->scikit-image->-r requirement.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: locket in /home/wschenst06g/.local/lib/python3.8/site-packages (from partd>=0.3.10->dask[array]>=0.9.0->scikit-image->-r requirement.txt (line 2)) (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc96f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/wschenst06g/.local/lib/python3.8/site-packages (0.1.26)\r\n",
      "Requirement already satisfied: torch>=1.0 in /home/wschenst06g/.local/lib/python3.8/site-packages (from timm) (1.10.1)\r\n",
      "Requirement already satisfied: torchvision in /home/wschenst06g/.local/lib/python3.8/site-packages (from timm) (0.11.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.0->timm) (3.10.0.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/wschenst06g/.local/lib/python3.8/site-packages (from torchvision->timm) (8.4.0)\r\n",
      "Requirement already satisfied: numpy in /home/wschenst06g/.local/lib/python3.8/site-packages (from torchvision->timm) (1.22.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b6db3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.1 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [1408 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]m\u001b[33m\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB] \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [821 kB]33m\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]    \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1118 kB]m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [891 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1837 kB]\u001b[33m\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.7 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [50.8 kB]33m\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [22.4 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [837 kB]\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Fetched 20.5 MB in 56s (363 kB/s)                                              \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libgl1 libgl1-mesa-dri libglapi-mesa libglvnd0 libglx-mesa0\n",
      "  libglx0 libllvm12 libpciaccess0 libsensors-config libsensors5 libvulkan1\n",
      "  libwayland-client0 libx11-xcb1 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0\n",
      "  libxcb-present0 libxcb-randr0 libxcb-shm0 libxcb-sync1 libxcb-xfixes0\n",
      "  libxfixes3 libxshmfence1 libxxf86vm1 mesa-vulkan-drivers\n",
      "Suggested packages:\n",
      "  pciutils lm-sensors\n",
      "The following NEW packages will be installed:\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libgl1 libgl1-mesa-dri libgl1-mesa-glx libglapi-mesa\n",
      "  libglvnd0 libglx-mesa0 libglx0 libllvm12 libpciaccess0 libsensors-config\n",
      "  libsensors5 libvulkan1 libwayland-client0 libx11-xcb1 libxcb-dri2-0\n",
      "  libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-randr0 libxcb-shm0\n",
      "  libxcb-sync1 libxcb-xfixes0 libxfixes3 libxshmfence1 libxxf86vm1\n",
      "  mesa-vulkan-drivers\n",
      "0 upgraded, 33 newly installed, 0 to remove and 55 not upgraded.\n",
      "Need to get 34.9 MB of archives.\n",
      "After this operation, 458 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libelf1 amd64 0.176-1.1build1 [44.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdrm-common all 2.4.105-3~20.04.2 [5552 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdrm2 amd64 2.4.105-3~20.04.2 [32.3 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdrm-amdgpu1 amd64 2.4.105-3~20.04.2 [18.4 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libpciaccess0 amd64 0.16-0ubuntu1 [17.9 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdrm-intel1 amd64 2.4.105-3~20.04.2 [60.7 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdrm-nouveau2 amd64 2.4.105-3~20.04.2 [16.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdrm-radeon1 amd64 2.4.105-3~20.04.2 [19.7 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglapi-mesa amd64 21.0.3-0ubuntu0.3~20.04.5 [26.8 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libllvm12 amd64 1:12.0.0-3ubuntu1~20.04.4 [18.8 MB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libsensors-config all 1:3.6.0-2ubuntu1 [6092 B]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libsensors5 amd64 1:3.6.0-2ubuntu1 [27.4 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libvulkan1 amd64 1.2.131.2-1 [93.3 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgl1-mesa-dri amd64 21.0.3-0ubuntu0.3~20.04.5 [10.5 MB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglvnd0 amd64 1.3.2-1~ubuntu0.20.04.1 [51.4 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libx11-xcb1 amd64 2:1.6.9-2ubuntu1.2 [9372 B]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-dri2-0 amd64 1.14-2 [6920 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-dri3-0 amd64 1.14-2 [6552 B]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-glx0 amd64 1.14-2 [22.1 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-present0 amd64 1.14-2 [5560 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-shm0 amd64 1.14-2 [5584 B]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-sync1 amd64 1.14-2 [8884 B]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-xfixes0 amd64 1.14-2 [9296 B]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal/main amd64 libxfixes3 amd64 1:5.0.3-2 [10.9 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal/main amd64 libxshmfence1 amd64 1.3-1 [5028 B]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86vm1 amd64 1:1.1.4-1build1 [10.2 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglx-mesa0 amd64 21.0.3-0ubuntu0.3~20.04.5 [138 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglx0 amd64 1.3.2-1~ubuntu0.20.04.1 [32.6 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgl1 amd64 1.3.2-1~ubuntu0.20.04.1 [86.9 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgl1-mesa-glx amd64 21.0.3-0ubuntu0.3~20.04.5 [5532 B]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu focal/main amd64 libwayland-client0 amd64 1.18.0-1 [23.9 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-randr0 amd64 1.14-2 [16.3 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 mesa-vulkan-drivers amd64 21.0.3-0ubuntu0.3~20.04.5 [4731 kB]\n",
      "Fetched 34.9 MB in 6s (5877 kB/s)              \u001b[0m\u001b[33m\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 33.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Extracting templates from packages: 100%\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libelf1:amd64.\n",
      "(Reading database ... 40531 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libelf1_0.176-1.1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libelf1:amd64 (0.176-1.1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [..........................................................] \u001b8Selecting previously unselected package libdrm-common.\n",
      "Preparing to unpack .../01-libdrm-common_2.4.105-3~20.04.2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libdrm-common (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Selecting previously unselected package libdrm2:amd64.\n",
      "Preparing to unpack .../02-libdrm2_2.4.105-3~20.04.2_amd64.deb ...\n",
      "Unpacking libdrm2:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package libdrm-amdgpu1:amd64.\n",
      "Preparing to unpack .../03-libdrm-amdgpu1_2.4.105-3~20.04.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libdrm-amdgpu1:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Selecting previously unselected package libpciaccess0:amd64.\n",
      "Preparing to unpack .../04-libpciaccess0_0.16-0ubuntu1_amd64.deb ...\n",
      "Unpacking libpciaccess0:amd64 (0.16-0ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libdrm-intel1:amd64.\n",
      "Preparing to unpack .../05-libdrm-intel1_2.4.105-3~20.04.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libdrm-intel1:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libdrm-nouveau2:amd64.\n",
      "Preparing to unpack .../06-libdrm-nouveau2_2.4.105-3~20.04.2_amd64.deb ...\n",
      "Unpacking libdrm-nouveau2:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libdrm-radeon1:amd64.\n",
      "Preparing to unpack .../07-libdrm-radeon1_2.4.105-3~20.04.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libdrm-radeon1:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libglapi-mesa:amd64.\n",
      "Preparing to unpack .../08-libglapi-mesa_21.0.3-0ubuntu0.3~20.04.5_amd64.deb ...\n",
      "Unpacking libglapi-mesa:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libllvm12:amd64.\n",
      "Preparing to unpack .../09-libllvm12_1%3a12.0.0-3ubuntu1~20.04.4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libllvm12:amd64 (1:12.0.0-3ubuntu1~20.04.4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libsensors-config.\n",
      "Preparing to unpack .../10-libsensors-config_1%3a3.6.0-2ubuntu1_all.deb ...\n",
      "Unpacking libsensors-config (1:3.6.0-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libsensors5:amd64.\n",
      "Preparing to unpack .../11-libsensors5_1%3a3.6.0-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libsensors5:amd64 (1:3.6.0-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libvulkan1:amd64.\n",
      "Preparing to unpack .../12-libvulkan1_1.2.131.2-1_amd64.deb ...\n",
      "Unpacking libvulkan1:amd64 (1.2.131.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libgl1-mesa-dri:amd64.\n",
      "Preparing to unpack .../13-libgl1-mesa-dri_21.0.3-0ubuntu0.3~20.04.5_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking libgl1-mesa-dri:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libglvnd0:amd64.\n",
      "Preparing to unpack .../14-libglvnd0_1.3.2-1~ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking libglvnd0:amd64 (1.3.2-1~ubuntu0.20.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libx11-xcb1:amd64.\n",
      "Preparing to unpack .../15-libx11-xcb1_2%3a1.6.9-2ubuntu1.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libx11-xcb1:amd64 (2:1.6.9-2ubuntu1.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libxcb-dri2-0:amd64.\n",
      "Preparing to unpack .../16-libxcb-dri2-0_1.14-2_amd64.deb ...\n",
      "Unpacking libxcb-dri2-0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libxcb-dri3-0:amd64.\n",
      "Preparing to unpack .../17-libxcb-dri3-0_1.14-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libxcb-dri3-0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libxcb-glx0:amd64.\n",
      "Preparing to unpack .../18-libxcb-glx0_1.14-2_amd64.deb ...\n",
      "Unpacking libxcb-glx0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libxcb-present0:amd64.\n",
      "Preparing to unpack .../19-libxcb-present0_1.14-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libxcb-present0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libxcb-shm0:amd64.\n",
      "Preparing to unpack .../20-libxcb-shm0_1.14-2_amd64.deb ...\n",
      "Unpacking libxcb-shm0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libxcb-sync1:amd64.\n",
      "Preparing to unpack .../21-libxcb-sync1_1.14-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libxcb-sync1:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libxcb-xfixes0:amd64.\n",
      "Preparing to unpack .../22-libxcb-xfixes0_1.14-2_amd64.deb ...\n",
      "Unpacking libxcb-xfixes0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libxfixes3:amd64.\n",
      "Preparing to unpack .../23-libxfixes3_1%3a5.0.3-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Unpacking libxfixes3:amd64 (1:5.0.3-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libxshmfence1:amd64.\n",
      "Preparing to unpack .../24-libxshmfence1_1.3-1_amd64.deb ...\n",
      "Unpacking libxshmfence1:amd64 (1.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libxxf86vm1:amd64.\n",
      "Preparing to unpack .../25-libxxf86vm1_1%3a1.1.4-1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libxxf86vm1:amd64 (1:1.1.4-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libglx-mesa0:amd64.\n",
      "Preparing to unpack .../26-libglx-mesa0_21.0.3-0ubuntu0.3~20.04.5_amd64.deb ...\n",
      "Unpacking libglx-mesa0:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package libglx0:amd64.\n",
      "Preparing to unpack .../27-libglx0_1.3.2-1~ubuntu0.20.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking libglx0:amd64 (1.3.2-1~ubuntu0.20.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libgl1:amd64.\n",
      "Preparing to unpack .../28-libgl1_1.3.2-1~ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking libgl1:amd64 (1.3.2-1~ubuntu0.20.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
      "Preparing to unpack .../29-libgl1-mesa-glx_21.0.3-0ubuntu0.3~20.04.5_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libgl1-mesa-glx:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Selecting previously unselected package libwayland-client0:amd64.\n",
      "Preparing to unpack .../30-libwayland-client0_1.18.0-1_amd64.deb ...\n",
      "Unpacking libwayland-client0:amd64 (1.18.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package libxcb-randr0:amd64.\n",
      "Preparing to unpack .../31-libxcb-randr0_1.14-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking libxcb-randr0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
      "Preparing to unpack .../32-mesa-vulkan-drivers_21.0.3-0ubuntu0.3~20.04.5_amd64.deb ...\n",
      "Unpacking mesa-vulkan-drivers:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up libxcb-dri3-0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libx11-xcb1:amd64 (2:1.6.9-2ubuntu1.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libpciaccess0:amd64 (0.16-0ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libxcb-xfixes0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libglvnd0:amd64 (1.3.2-1~ubuntu0.20.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libxcb-glx0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libsensors-config (1:3.6.0-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libxcb-shm0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libxxf86vm1:amd64 (1:1.1.4-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libxcb-present0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libxfixes3:amd64 (1:5.0.3-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libxcb-sync1:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libllvm12:amd64 (1:12.0.0-3ubuntu1~20.04.4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libsensors5:amd64 (1:3.6.0-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libglapi-mesa:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libvulkan1:amd64 (1.2.131.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libxcb-dri2-0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libxshmfence1:amd64 (1.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libxcb-randr0:amd64 (1.14-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libdrm-common (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libelf1:amd64 (0.176-1.1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libwayland-client0:amd64 (1.18.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libdrm2:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libdrm-amdgpu1:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up mesa-vulkan-drivers:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libdrm-nouveau2:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libdrm-radeon1:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libdrm-intel1:amd64 (2.4.105-3~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libgl1-mesa-dri:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libglx-mesa0:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libglx0:amd64 (1.3.2-1~ubuntu0.20.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up libgl1:amd64 (1.3.2-1~ubuntu0.20.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libgl1-mesa-glx:amd64 (21.0.3-0ubuntu0.3~20.04.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [#########################################################.] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8Processing triggers for libc-bin (2.31-0ubuntu9.2) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install libgl1-mesa-glx -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b048a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
      "Collecting torch==1.8.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/lts/1.8/cu111/torch-1.8.1%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
      "     |████████████████████████████████| 1982.2 MB 26 kB/s                     | 1.2 MB 201 kB/s eta 2:43:35                             | 27.4 MB 717 kB/s eta 0:45:23MB 984 kB/s eta 0:32:36                             | 61.7 MB 984 kB/s eta 0:32:32MB 984 kB/s eta 0:32:28██▏                        | 441.5 MB 3.8 MB/s eta 0:06:47��████                       | 559.2 MB 203.9 MB/s eta 0:00:07s eta 0:06:25 ��█████████████▊             | 1156.4 MB 3.5 MB/s eta 0:03:55 ��██           | 1307.1 MB 4.1 MB/s eta 0:02:45  201.6 MB/s eta 0:00:03�███████████▋        | 1462.8 MB 25.4 MB/s eta 0:00:21 \n",
      "\u001b[?25hCollecting torchvision==0.9.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/lts/1.8/cu111/torchvision-0.9.1%2Bcu111-cp38-cp38-linux_x86_64.whl (17.6 MB)\n",
      "     |████████████████████████████████| 17.6 MB 706 kB/s                      | 9.4 MB 706 kB/s eta 0:00:12�██████████         | 12.7 MB 706 kB/s eta 0:00:07\n",
      "\u001b[?25hRequirement already satisfied: torchaudio==0.8.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in /home/wschenst06g/.local/lib/python3.8/site-packages (from torch==1.8.1+cu111) (1.22.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.8.1+cu111) (3.10.0.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (from torchvision==0.9.1+cu111) (8.4.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.1\n",
      "    Uninstalling torch-1.10.1:\n",
      "      Successfully uninstalled torch-1.10.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.11.2\n",
      "    Uninstalling torchvision-0.11.2:\n",
      "      Successfully uninstalled torchvision-0.11.2\n",
      "Successfully installed torch-1.8.1+cu111 torchvision-0.9.1+cu111\n",
      "Requirement already satisfied: torchtext==0.9.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchtext==0.9.1) (2.26.0)\n",
      "Requirement already satisfied: numpy in /home/wschenst06g/.local/lib/python3.8/site-packages (from torchtext==0.9.1) (1.22.0)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (from torchtext==0.9.1) (1.8.1+cu111)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from torchtext==0.9.1) (4.62.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.8.1->torchtext==0.9.1) (3.10.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.9.1) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.9.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (from requests->torchtext==0.9.1) (1.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.9.1) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
    "!pip install torchtext==0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d259dd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/wschenst06g/.local/lib/python3.8/site-packages (0.1.26)\n",
      "Collecting timm\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "     |████████████████████████████████| 376 kB 959 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4 in /home/wschenst06g/.local/lib/python3.8/site-packages (from timm) (1.8.1+cu111)\n",
      "Requirement already satisfied: torchvision in /home/wschenst06g/.local/lib/python3.8/site-packages (from timm) (0.9.1+cu111)\n",
      "Requirement already satisfied: numpy in /home/wschenst06g/.local/lib/python3.8/site-packages (from torch>=1.4->timm) (1.22.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm) (3.10.0.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/wschenst06g/.local/lib/python3.8/site-packages (from torchvision->timm) (8.4.0)\n",
      "Installing collected packages: timm\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.1.26\n",
      "    Uninstalling timm-0.1.26:\n",
      "      Successfully uninstalled timm-0.1.26\n",
      "Successfully installed timm-0.4.12\n"
     ]
    }
   ],
   "source": [
    "!pip install -U timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68ddec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "number of GPUs is: 8\n",
      "LogHandlers setup!\n",
      "22-01-11 22:57:48.915 :   task: swinir_sr_classical_patch48_x3\n",
      "  model: plain\n",
      "  gpu_ids: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "  dist: False\n",
      "  scale: 3\n",
      "  n_channels: 3\n",
      "  path:[\n",
      "    root: superresolution\n",
      "    pretrained_netG: None\n",
      "    pretrained_netE: None\n",
      "    task: superresolution/swinir_sr_classical_patch48_x3\n",
      "    log: superresolution/swinir_sr_classical_patch48_x3\n",
      "    options: superresolution/swinir_sr_classical_patch48_x3/options\n",
      "    models: superresolution/swinir_sr_classical_patch48_x3/models\n",
      "    images: superresolution/swinir_sr_classical_patch48_x3/images\n",
      "    pretrained_optimizerG: None\n",
      "  ]\n",
      "  datasets:[\n",
      "    train:[\n",
      "      name: train_dataset\n",
      "      dataset_type: sr\n",
      "      dataroot_H: trainsets/trainH\n",
      "      dataroot_L: None\n",
      "      H_size: 96\n",
      "      dataloader_shuffle: True\n",
      "      dataloader_num_workers: 16\n",
      "      dataloader_batch_size: 16\n",
      "      phase: train\n",
      "      scale: 3\n",
      "      n_channels: 3\n",
      "    ]\n",
      "    test:[\n",
      "      name: test_dataset\n",
      "      dataset_type: sr\n",
      "      dataroot_H: trainsets/val\n",
      "      dataroot_L: None\n",
      "      phase: test\n",
      "      scale: 3\n",
      "      n_channels: 3\n",
      "    ]\n",
      "  ]\n",
      "  netG:[\n",
      "    net_type: swinir\n",
      "    upscale: 3\n",
      "    in_chans: 3\n",
      "    img_size: 48\n",
      "    window_size: 8\n",
      "    img_range: 1.0\n",
      "    depths: [6, 6, 6, 6, 6, 6]\n",
      "    embed_dim: 180\n",
      "    num_heads: [6, 6, 6, 6, 6, 6]\n",
      "    mlp_ratio: 2\n",
      "    upsampler: pixelshuffle\n",
      "    resi_connection: 1conv\n",
      "    init_type: default\n",
      "    scale: 3\n",
      "  ]\n",
      "  train:[\n",
      "    G_lossfn_type: l1\n",
      "    G_lossfn_weight: 1.2\n",
      "    E_decay: 0.999\n",
      "    G_optimizer_type: adam\n",
      "    G_optimizer_lr: 0.0002\n",
      "    G_optimizer_wd: 0\n",
      "    G_optimizer_clipgrad: None\n",
      "    G_optimizer_reuse: True\n",
      "    G_scheduler_type: MultiStepLR\n",
      "    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]\n",
      "    G_scheduler_gamma: 0.5\n",
      "    G_regularizer_orthstep: None\n",
      "    G_regularizer_clipstep: None\n",
      "    G_param_strict: True\n",
      "    E_param_strict: True\n",
      "    checkpoint_test: 5000\n",
      "    checkpoint_save: 5000\n",
      "    checkpoint_print: 500\n",
      "    F_feature_layer: 34\n",
      "    F_weights: 1.0\n",
      "    F_lossfn_type: l1\n",
      "    F_use_input_norm: True\n",
      "    F_use_range_norm: False\n",
      "  ]\n",
      "  opt_path: options/swinir/train_swinir_sr_classical.json\n",
      "  is_train: True\n",
      "  merge_bn: False\n",
      "  merge_bn_startpoint: -1\n",
      "  find_unused_parameters: True\n",
      "  num_gpu: 8\n",
      "  rank: 0\n",
      "  world_size: 1\n",
      "\n",
      "Random seed: 9299\n",
      "Dataset [DatasetSR - train_dataset] is created.\n",
      "22-01-11 22:57:48.917 : Number of train images: 280, iters: 18\n",
      "/home/wschenst06g/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Dataset [DatasetSR - test_dataset] is created.\n",
      "Pass this initialization! Initialization was done during network defination!\n",
      "Pass this initialization! Initialization was done during network defination!\n",
      "Training model [ModelPlain] is created.\n",
      "Copying model for E ...\n",
      "22-01-11 22:57:52.626 : \n",
      "Networks name: SwinIR\n",
      "Params number: 11937127\n",
      "Net structure:\n",
      "SwinIR(\n",
      "  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (patch_unembed): PatchUnEmbed()\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=180, input_resolution=(48, 48), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (1): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=180, input_resolution=(48, 48), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (2): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=180, input_resolution=(48, 48), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (3): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=180, input_resolution=(48, 48), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (4): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=180, input_resolution=(48, 48), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (5): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=180, input_resolution=(48, 48), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=180, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\r\n",
      "              dim=180, window_size=(8, 8), num_heads=6\r\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\r\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\r\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (softmax): Softmax(dim=-1)\r\n",
      "            )\r\n",
      "            (drop_path): DropPath()\r\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (mlp): Mlp(\r\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\r\n",
      "              (act): GELU()\r\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\r\n",
      "              (drop): Dropout(p=0.0, inplace=False)\r\n",
      "            )\r\n",
      "          )\r\n",
      "          (2): SwinTransformerBlock(\r\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\r\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (attn): WindowAttention(\r\n",
      "              dim=180, window_size=(8, 8), num_heads=6\r\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\r\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\r\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (softmax): Softmax(dim=-1)\r\n",
      "            )\r\n",
      "            (drop_path): DropPath()\r\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (mlp): Mlp(\r\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\r\n",
      "              (act): GELU()\r\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\r\n",
      "              (drop): Dropout(p=0.0, inplace=False)\r\n",
      "            )\r\n",
      "          )\r\n",
      "          (3): SwinTransformerBlock(\r\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\r\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (attn): WindowAttention(\r\n",
      "              dim=180, window_size=(8, 8), num_heads=6\r\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\r\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\r\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (softmax): Softmax(dim=-1)\r\n",
      "            )\r\n",
      "            (drop_path): DropPath()\r\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (mlp): Mlp(\r\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\r\n",
      "              (act): GELU()\r\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\r\n",
      "              (drop): Dropout(p=0.0, inplace=False)\r\n",
      "            )\r\n",
      "          )\r\n",
      "          (4): SwinTransformerBlock(\r\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\r\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (attn): WindowAttention(\r\n",
      "              dim=180, window_size=(8, 8), num_heads=6\r\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\r\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\r\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (softmax): Softmax(dim=-1)\r\n",
      "            )\r\n",
      "            (drop_path): DropPath()\r\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (mlp): Mlp(\r\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\r\n",
      "              (act): GELU()\r\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\r\n",
      "              (drop): Dropout(p=0.0, inplace=False)\r\n",
      "            )\r\n",
      "          )\r\n",
      "          (5): SwinTransformerBlock(\r\n",
      "            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\r\n",
      "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (attn): WindowAttention(\r\n",
      "              dim=180, window_size=(8, 8), num_heads=6\r\n",
      "              (qkv): Linear(in_features=180, out_features=540, bias=True)\r\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (proj): Linear(in_features=180, out_features=180, bias=True)\r\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\r\n",
      "              (softmax): Softmax(dim=-1)\r\n",
      "            )\r\n",
      "            (drop_path): DropPath()\r\n",
      "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "            (mlp): Mlp(\r\n",
      "              (fc1): Linear(in_features=180, out_features=360, bias=True)\r\n",
      "              (act): GELU()\r\n",
      "              (fc2): Linear(in_features=360, out_features=180, bias=True)\r\n",
      "              (drop): Dropout(p=0.0, inplace=False)\r\n",
      "            )\r\n",
      "          )\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n",
      "      (patch_embed): PatchEmbed()\r\n",
      "      (patch_unembed): PatchUnEmbed()\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\r\n",
      "  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n",
      "  (conv_before_upsample): Sequential(\r\n",
      "    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\r\n",
      "  )\r\n",
      "  (upsample): Upsample(\r\n",
      "    (0): Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n",
      "    (1): PixelShuffle(upscale_factor=3)\r\n",
      "  )\r\n",
      "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n",
      ")\r\n",
      "\r\n",
      "22-01-11 22:57:52.702 : \r\n",
      " |  mean  |  min   |  max   |  std   || shape               \r\n",
      " | -0.001 | -0.192 |  0.192 |  0.110 | torch.Size([180, 3, 3, 3]) || conv_first.weight\r\n",
      " | -0.011 | -0.192 |  0.191 |  0.115 | torch.Size([180]) || conv_first.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias\r\n",
      " | -0.000 | -0.061 |  0.075 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index\r\n",
      " |  0.000 | -0.091 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias\r\n",
      " | -0.000 | -0.097 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias\r\n",
      " | -0.000 | -0.084 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias\r\n",
      " |  0.000 | -0.087 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias\r\n",
      " |  0.000 | -0.074 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index\r\n",
      " |  0.000 | -0.086 |  0.080 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias\r\n",
      " | -0.000 | -0.086 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias\r\n",
      " | -0.000 | -0.092 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias\r\n",
      " | -0.000 | -2.000 |  0.092 |  0.021 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias\r\n",
      " |  0.000 | -0.067 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index\r\n",
      " |  0.000 | -0.084 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias\r\n",
      " |  0.000 | -0.077 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias\r\n",
      " | -0.000 | -0.094 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias\r\n",
      " |  0.000 | -0.101 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias\r\n",
      " |  0.000 | -0.056 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index\r\n",
      " | -0.000 | -0.084 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias\r\n",
      " |  0.000 | -0.086 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias\r\n",
      " | -0.000 | -0.087 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias\r\n",
      " | -0.000 | -0.081 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias\r\n",
      " |  0.001 | -0.060 |  0.077 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index\r\n",
      " | -0.000 | -0.093 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias\r\n",
      " | -0.000 | -0.085 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias\r\n",
      " | -0.000 | -0.077 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias\r\n",
      " | -0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias\r\n",
      " |  0.001 | -0.061 |  0.057 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index\r\n",
      " | -0.000 | -0.087 |  0.099 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias\r\n",
      " |  0.000 | -0.078 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias\r\n",
      " |  0.000 | -0.086 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias\r\n",
      " |  0.000 | -0.087 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias\r\n",
      " | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight\r\n",
      " | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.0.conv.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias\r\n",
      " | -0.001 | -0.069 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index\r\n",
      " | -0.000 | -0.103 |  0.079 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias\r\n",
      " |  0.000 | -0.075 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias\r\n",
      " |  0.000 | -0.077 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias\r\n",
      " |  0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias\r\n",
      " | -0.000 | -0.064 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index\r\n",
      " | -0.000 | -0.081 |  0.103 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias\r\n",
      " |  0.000 | -0.081 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias\r\n",
      " | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias\r\n",
      " | -0.000 | -0.087 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias\r\n",
      " |  0.000 | -0.065 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index\r\n",
      " | -0.000 | -0.090 |  0.096 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias\r\n",
      " | -0.000 | -0.083 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias\r\n",
      " | -0.000 | -0.082 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias\r\n",
      " | -0.000 | -0.087 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias\r\n",
      " | -0.001 | -0.066 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index\r\n",
      " | -0.000 | -0.081 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias\r\n",
      " | -0.000 | -0.077 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias\r\n",
      " |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias\r\n",
      " | -0.000 | -0.091 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias\r\n",
      " |  0.000 | -0.073 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index\r\n",
      " | -0.000 | -0.086 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias\r\n",
      " |  0.000 | -0.085 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias\r\n",
      " | -0.000 | -0.091 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias\r\n",
      " | -0.000 | -0.078 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias\r\n",
      " |  0.001 | -0.071 |  0.086 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index\r\n",
      " |  0.000 | -0.086 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias\r\n",
      " | -0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias\r\n",
      " |  0.000 | -0.094 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias\r\n",
      " | -0.000 | -0.084 |  0.099 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias\r\n",
      " | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight\r\n",
      " | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.1.conv.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias\r\n",
      " |  0.000 | -0.058 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index\r\n",
      " | -0.000 | -0.087 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias\r\n",
      " |  0.000 | -0.077 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias\r\n",
      " | -0.000 | -0.083 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias\r\n",
      " | -0.000 | -0.082 |  0.093 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias\r\n",
      " | -0.000 | -0.058 |  0.068 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index\r\n",
      " |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias\r\n",
      " |  0.000 | -0.088 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias\r\n",
      " | -0.000 | -0.090 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias\r\n",
      " |  0.000 | -0.088 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias\r\n",
      " | -0.001 | -0.063 |  0.069 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index\r\n",
      " | -0.000 | -0.084 |  0.080 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias\r\n",
      " | -0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias\r\n",
      " | -0.000 | -0.079 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias\r\n",
      " |  0.000 | -0.092 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias\r\n",
      " | -0.001 | -0.072 |  0.061 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index\r\n",
      " |  0.000 | -0.087 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias\r\n",
      " |  0.000 | -0.083 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias\r\n",
      " | -0.000 | -0.081 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias\r\n",
      " | -0.000 | -0.088 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias\r\n",
      " |  0.000 | -0.059 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index\r\n",
      " | -0.000 | -0.095 |  0.100 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias\r\n",
      " |  0.000 | -0.075 |  0.092 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias\r\n",
      " |  0.000 | -0.077 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias\r\n",
      " | -0.000 | -0.079 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias\r\n",
      " |  0.000 | -0.053 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index\r\n",
      " |  0.000 | -0.099 |  0.099 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias\r\n",
      " | -0.000 | -0.076 |  0.073 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias\r\n",
      " |  0.000 | -0.095 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias\r\n",
      " |  0.000 | -0.086 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias\r\n",
      " |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight\r\n",
      " |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.2.conv.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias\r\n",
      " |  0.000 | -0.063 |  0.055 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index\r\n",
      " |  0.000 | -0.082 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias\r\n",
      " |  0.000 | -0.084 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias\r\n",
      " | -0.000 | -0.087 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias\r\n",
      " |  0.000 | -0.081 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias\r\n",
      " | -0.000 | -0.065 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index\r\n",
      " | -0.000 | -0.090 |  0.079 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias\r\n",
      " |  0.000 | -0.081 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias\r\n",
      " | -0.000 | -0.089 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias\r\n",
      " |  0.000 | -0.078 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias\r\n",
      " | -0.000 | -0.065 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index\r\n",
      " | -0.000 | -0.081 |  0.080 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias\r\n",
      " |  0.000 | -0.078 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias\r\n",
      " | -0.000 | -0.084 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias\r\n",
      " |  0.000 | -0.087 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias\r\n",
      " |  0.001 | -0.074 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index\r\n",
      " | -0.000 | -0.085 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias\r\n",
      " |  0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias\r\n",
      " | -0.000 | -0.086 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias\r\n",
      " |  0.000 | -0.103 |  0.076 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias\r\n",
      " |  0.000 | -0.066 |  0.058 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index\r\n",
      " |  0.000 | -0.085 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias\r\n",
      " |  0.000 | -0.077 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias\r\n",
      " |  0.000 | -0.092 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias\r\n",
      " | -0.000 | -0.078 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias\r\n",
      " | -0.001 | -0.076 |  0.087 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index\r\n",
      " | -0.000 | -0.083 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias\r\n",
      " |  0.000 | -0.082 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias\r\n",
      " | -0.000 | -0.076 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias\r\n",
      " | -0.000 | -0.084 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias\r\n",
      " |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight\r\n",
      " | -0.000 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.3.conv.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias\r\n",
      " | -0.000 | -0.068 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index\r\n",
      " | -0.000 | -0.106 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias\r\n",
      " |  0.000 | -0.090 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias\r\n",
      " |  0.000 | -0.084 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias\r\n",
      " | -0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias\r\n",
      " |  0.000 | -0.069 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index\r\n",
      " | -0.000 | -0.086 |  0.102 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias\r\n",
      " |  0.000 | -0.079 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias\r\n",
      " | -0.000 | -0.090 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias\r\n",
      " | -0.000 | -0.094 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias\r\n",
      " | -0.000 | -0.065 |  0.084 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index\r\n",
      " |  0.000 | -0.084 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias\r\n",
      " |  0.000 | -0.074 |  0.092 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias\r\n",
      " |  0.000 | -0.077 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias\r\n",
      " | -0.000 | -0.086 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias\r\n",
      " |  0.001 | -0.087 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index\r\n",
      " |  0.000 | -0.089 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias\r\n",
      " |  0.000 | -0.074 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias\r\n",
      " |  0.000 | -0.086 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias\r\n",
      " | -0.000 | -0.082 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias\r\n",
      " | -0.001 | -0.062 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index\r\n",
      " | -0.000 | -0.080 |  0.078 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias\r\n",
      " |  0.000 | -0.086 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias\r\n",
      " | -0.000 | -0.075 |  0.101 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias\r\n",
      " | -0.000 | -0.092 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias\r\n",
      " |  0.001 | -0.059 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index\r\n",
      " | -0.000 | -0.098 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias\r\n",
      " |  0.000 | -0.085 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias\r\n",
      " | -0.000 | -0.093 |  0.094 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias\r\n",
      " | -0.000 | -0.085 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias\r\n",
      " |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight\r\n",
      " | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.4.conv.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias\r\n",
      " | -0.000 | -0.068 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index\r\n",
      " | -0.000 | -0.101 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias\r\n",
      " |  0.000 | -0.089 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias\r\n",
      " | -0.000 | -0.077 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias\r\n",
      " | -0.000 | -0.079 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias\r\n",
      " |  0.000 | -0.076 |  0.057 |  0.019 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index\r\n",
      " | -0.000 | -0.081 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias\r\n",
      " |  0.000 | -0.084 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias\r\n",
      " | -0.000 | -0.077 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias\r\n",
      " |  0.000 | -0.087 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias\r\n",
      " | -0.002 | -0.086 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index\r\n",
      " | -0.000 | -0.090 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias\r\n",
      " | -0.000 | -0.072 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias\r\n",
      " |  0.000 | -0.089 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias\r\n",
      " | -0.000 | -0.082 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias\r\n",
      " | -0.001 | -0.072 |  0.056 |  0.019 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index\r\n",
      " | -0.000 | -0.084 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias\r\n",
      " | -0.000 | -0.085 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias\r\n",
      " | -0.000 | -0.087 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias\r\n",
      " |  0.000 | -0.082 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias\r\n",
      " |  0.000 | -0.076 |  0.075 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index\r\n",
      " |  0.000 | -0.085 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias\r\n",
      " |  0.000 | -0.089 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias\r\n",
      " | -0.000 | -0.088 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias\r\n",
      " | -0.000 | -0.084 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias\r\n",
      " | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias\r\n",
      " | -0.001 | -0.058 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table\r\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index\r\n",
      " |  0.000 | -0.098 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias\r\n",
      " | -0.000 | -0.086 |  0.096 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias\r\n",
      " |  0.000 | -0.085 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias\r\n",
      " |  0.000 | -0.090 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias\r\n",
      " |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight\r\n",
      " |  0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.5.conv.bias\r\n",
      " |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight\r\n",
      " |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias\r\n",
      " | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight\r\n",
      " | -0.000 | -0.025 |  0.025 |  0.013 | torch.Size([180]) || conv_after_body.bias\r\n",
      " | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight\r\n",
      " |  0.003 | -0.025 |  0.025 |  0.013 | torch.Size([64]) || conv_before_upsample.0.bias\r\n",
      " | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([576, 64, 3, 3]) || upsample.0.weight\r\n",
      " |  0.001 | -0.042 |  0.042 |  0.024 | torch.Size([576]) || upsample.0.bias\r\n",
      " |  0.001 | -0.042 |  0.042 |  0.024 | torch.Size([3, 64, 3, 3]) || conv_last.weight\r\n",
      " | -0.016 | -0.030 |  0.000 |  0.015 | torch.Size([3]) || conv_last.bias\r\n",
      "\r\n",
      "/home/wschenst06g/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/wschenst06g/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/wschenst06g/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:416: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "22-01-11 23:02:18.888 : <epoch: 29, iter:     500, lr:2.000e-04> G_loss: 5.129e-02 \n",
      "22-01-11 23:06:40.358 : <epoch: 58, iter:   1,000, lr:2.000e-04> G_loss: 3.044e-02 \n",
      "22-01-11 23:11:05.091 : <epoch: 88, iter:   1,500, lr:2.000e-04> G_loss: 4.457e-02 \n",
      "22-01-11 23:15:27.092 : <epoch:117, iter:   2,000, lr:2.000e-04> G_loss: 3.655e-02 \n",
      "22-01-11 23:19:51.962 : <epoch:147, iter:   2,500, lr:2.000e-04> G_loss: 3.210e-02 \n",
      "22-01-11 23:24:13.117 : <epoch:176, iter:   3,000, lr:2.000e-04> G_loss: 4.560e-02 \n",
      "22-01-11 23:28:35.669 : <epoch:205, iter:   3,500, lr:2.000e-04> G_loss: 4.442e-02 \n",
      "22-01-11 23:33:00.495 : <epoch:235, iter:   4,000, lr:2.000e-04> G_loss: 3.107e-02 \n",
      "22-01-11 23:37:21.274 : <epoch:264, iter:   4,500, lr:2.000e-04> G_loss: 3.524e-02 \n",
      "22-01-11 23:41:45.712 : <epoch:294, iter:   5,000, lr:2.000e-04> G_loss: 3.483e-02 \n",
      "22-01-11 23:41:45.712 : Saving the model.\n",
      "22-01-11 23:41:46.459 : ---1-->   tt25.png | 30.85dB\n",
      "22-01-11 23:41:46.602 : ---2-->   tt26.png | 29.84dB\n",
      "22-01-11 23:41:46.730 : ---3-->   tt27.png | 33.30dB\n",
      "22-01-11 23:41:46.899 : ---4-->    tt3.png | 31.82dB\n",
      "22-01-11 23:41:47.060 : ---5-->    tt4.png | 32.41dB\n",
      "22-01-11 23:41:47.198 : ---6-->    tt5.png | 30.11dB\n",
      "22-01-11 23:41:47.379 : ---7-->    tt6.png | 24.84dB\n",
      "22-01-11 23:41:47.494 : ---8-->    tt7.png | 26.11dB\n",
      "22-01-11 23:41:47.616 : ---9-->    tt8.png | 33.50dB\n",
      "22-01-11 23:41:47.753 : --10-->    tt9.png | 33.21dB\n",
      "22-01-11 23:41:47.814 : <epoch:294, iter:   5,000, Average PSNR : 30.60dB\n",
      "\n",
      "22-01-11 23:46:09.415 : <epoch:323, iter:   5,500, lr:2.000e-04> G_loss: 3.025e-02 \n",
      "22-01-11 23:50:30.835 : <epoch:352, iter:   6,000, lr:2.000e-04> G_loss: 3.491e-02 \n",
      "22-01-11 23:54:55.333 : <epoch:382, iter:   6,500, lr:2.000e-04> G_loss: 3.049e-02 \n",
      "22-01-11 23:59:17.083 : <epoch:411, iter:   7,000, lr:2.000e-04> G_loss: 2.681e-02 \n",
      "22-01-12 00:03:41.840 : <epoch:441, iter:   7,500, lr:2.000e-04> G_loss: 3.601e-02 \n",
      "22-01-12 00:08:03.563 : <epoch:470, iter:   8,000, lr:2.000e-04> G_loss: 3.249e-02 \n",
      "22-01-12 00:12:24.835 : <epoch:499, iter:   8,500, lr:2.000e-04> G_loss: 2.588e-02 \n",
      "22-01-12 00:16:49.352 : <epoch:529, iter:   9,000, lr:2.000e-04> G_loss: 3.836e-02 \n",
      "22-01-12 00:21:10.268 : <epoch:558, iter:   9,500, lr:2.000e-04> G_loss: 3.564e-02 \n",
      "22-01-12 00:25:35.115 : <epoch:588, iter:  10,000, lr:2.000e-04> G_loss: 4.208e-02 \n",
      "22-01-12 00:25:35.115 : Saving the model.\n",
      "22-01-12 00:25:35.897 : ---1-->   tt25.png | 31.58dB\n",
      "22-01-12 00:25:36.044 : ---2-->   tt26.png | 30.99dB\n",
      "22-01-12 00:25:36.208 : ---3-->   tt27.png | 34.12dB\n",
      "22-01-12 00:25:36.387 : ---4-->    tt3.png | 32.64dB\n",
      "22-01-12 00:25:36.548 : ---5-->    tt4.png | 33.00dB\n",
      "22-01-12 00:25:36.684 : ---6-->    tt5.png | 30.80dB\n",
      "22-01-12 00:25:36.830 : ---7-->    tt6.png | 25.92dB\n",
      "22-01-12 00:25:36.931 : ---8-->    tt7.png | 27.12dB\n",
      "22-01-12 00:25:37.049 : ---9-->    tt8.png | 33.86dB\n",
      "22-01-12 00:25:37.184 : --10-->    tt9.png | 33.98dB\n",
      "22-01-12 00:25:37.244 : <epoch:588, iter:  10,000, Average PSNR : 31.40dB\n",
      "\n",
      "22-01-12 00:29:59.622 : <epoch:617, iter:  10,500, lr:2.000e-04> G_loss: 2.859e-02 \n",
      "22-01-12 00:34:23.635 : <epoch:647, iter:  11,000, lr:2.000e-04> G_loss: 3.850e-02 \n",
      "22-01-12 00:38:44.476 : <epoch:676, iter:  11,500, lr:2.000e-04> G_loss: 4.091e-02 \n",
      "22-01-12 00:43:05.308 : <epoch:705, iter:  12,000, lr:2.000e-04> G_loss: 3.714e-02 \n",
      "22-01-12 00:47:30.630 : <epoch:735, iter:  12,500, lr:2.000e-04> G_loss: 3.600e-02 \n",
      "22-01-12 00:51:52.323 : <epoch:764, iter:  13,000, lr:2.000e-04> G_loss: 3.616e-02 \n",
      "22-01-12 00:56:15.532 : <epoch:794, iter:  13,500, lr:2.000e-04> G_loss: 3.244e-02 \n",
      "22-01-12 01:00:36.624 : <epoch:823, iter:  14,000, lr:2.000e-04> G_loss: 3.275e-02 \n",
      "22-01-12 01:04:58.431 : <epoch:852, iter:  14,500, lr:2.000e-04> G_loss: 3.006e-02 \n",
      "22-01-12 01:09:23.292 : <epoch:882, iter:  15,000, lr:2.000e-04> G_loss: 2.795e-02 \n",
      "22-01-12 01:09:23.293 : Saving the model.\n",
      "22-01-12 01:09:24.126 : ---1-->   tt25.png | 31.95dB\n",
      "22-01-12 01:09:24.269 : ---2-->   tt26.png | 31.25dB\n",
      "22-01-12 01:09:24.404 : ---3-->   tt27.png | 34.52dB\n",
      "22-01-12 01:09:24.582 : ---4-->    tt3.png | 32.99dB\n",
      "22-01-12 01:09:24.757 : ---5-->    tt4.png | 33.83dB\n",
      "22-01-12 01:09:24.922 : ---6-->    tt5.png | 31.09dB\n",
      "22-01-12 01:09:25.067 : ---7-->    tt6.png | 26.32dB\n",
      "22-01-12 01:09:25.166 : ---8-->    tt7.png | 27.47dB\n",
      "22-01-12 01:09:25.283 : ---9-->    tt8.png | 34.85dB\n",
      "22-01-12 01:09:25.444 : --10-->    tt9.png | 34.49dB\n",
      "22-01-12 01:09:25.502 : <epoch:882, iter:  15,000, Average PSNR : 31.87dB\n",
      "\n",
      "22-01-12 01:13:47.170 : <epoch:911, iter:  15,500, lr:2.000e-04> G_loss: 3.276e-02 \n",
      "22-01-12 01:18:12.126 : <epoch:941, iter:  16,000, lr:2.000e-04> G_loss: 3.100e-02 \n",
      "22-01-12 01:22:34.440 : <epoch:970, iter:  16,500, lr:2.000e-04> G_loss: 3.097e-02 \n",
      "22-01-12 01:26:55.989 : <epoch:999, iter:  17,000, lr:2.000e-04> G_loss: 2.594e-02 \n",
      "22-01-12 01:31:22.260 : <epoch:1029, iter:  17,500, lr:2.000e-04> G_loss: 2.523e-02 \n",
      "22-01-12 01:35:44.280 : <epoch:1058, iter:  18,000, lr:2.000e-04> G_loss: 3.095e-02 \n",
      "22-01-12 01:40:09.902 : <epoch:1088, iter:  18,500, lr:2.000e-04> G_loss: 2.791e-02 \n",
      "22-01-12 01:44:32.151 : <epoch:1117, iter:  19,000, lr:2.000e-04> G_loss: 3.409e-02 \n",
      "22-01-12 01:48:57.278 : <epoch:1147, iter:  19,500, lr:2.000e-04> G_loss: 2.631e-02 \n",
      "22-01-12 01:53:19.118 : <epoch:1176, iter:  20,000, lr:2.000e-04> G_loss: 2.330e-02 \n",
      "22-01-12 01:53:19.118 : Saving the model.\n",
      "22-01-12 01:53:19.899 : ---1-->   tt25.png | 32.12dB\n",
      "22-01-12 01:53:20.047 : ---2-->   tt26.png | 31.27dB\n",
      "22-01-12 01:53:20.176 : ---3-->   tt27.png | 34.90dB\n",
      "22-01-12 01:53:20.347 : ---4-->    tt3.png | 33.07dB\n",
      "22-01-12 01:53:20.557 : ---5-->    tt4.png | 33.91dB\n",
      "22-01-12 01:53:20.694 : ---6-->    tt5.png | 31.08dB\n",
      "22-01-12 01:53:20.841 : ---7-->    tt6.png | 26.37dB\n",
      "22-01-12 01:53:20.940 : ---8-->    tt7.png | 27.70dB\n",
      "22-01-12 01:53:21.060 : ---9-->    tt8.png | 34.98dB\n",
      "22-01-12 01:53:21.195 : --10-->    tt9.png | 34.50dB\n",
      "22-01-12 01:53:21.255 : <epoch:1176, iter:  20,000, Average PSNR : 31.99dB\n",
      "\n",
      "22-01-12 01:57:42.550 : <epoch:1205, iter:  20,500, lr:2.000e-04> G_loss: 3.057e-02 \n",
      "22-01-12 02:02:06.538 : <epoch:1235, iter:  21,000, lr:2.000e-04> G_loss: 2.421e-02 \n",
      "22-01-12 02:06:28.348 : <epoch:1264, iter:  21,500, lr:2.000e-04> G_loss: 2.963e-02 \n",
      "22-01-12 02:10:54.253 : <epoch:1294, iter:  22,000, lr:2.000e-04> G_loss: 2.369e-02 \n",
      "22-01-12 02:15:15.913 : <epoch:1323, iter:  22,500, lr:2.000e-04> G_loss: 3.424e-02 \n",
      "22-01-12 02:19:37.344 : <epoch:1352, iter:  23,000, lr:2.000e-04> G_loss: 3.558e-02 \n",
      "22-01-12 02:24:03.774 : <epoch:1382, iter:  23,500, lr:2.000e-04> G_loss: 2.918e-02 \n",
      "22-01-12 02:28:25.355 : <epoch:1411, iter:  24,000, lr:2.000e-04> G_loss: 2.843e-02 \n",
      "22-01-12 02:32:50.400 : <epoch:1441, iter:  24,500, lr:2.000e-04> G_loss: 3.425e-02 \n",
      "22-01-12 02:37:12.324 : <epoch:1470, iter:  25,000, lr:2.000e-04> G_loss: 4.134e-02 \n",
      "22-01-12 02:37:12.324 : Saving the model.\n",
      "22-01-12 02:37:13.075 : ---1-->   tt25.png | 32.10dB\n",
      "22-01-12 02:37:13.240 : ---2-->   tt26.png | 31.31dB\n",
      "22-01-12 02:37:13.370 : ---3-->   tt27.png | 34.96dB\n",
      "22-01-12 02:37:13.538 : ---4-->    tt3.png | 33.14dB\n",
      "22-01-12 02:37:13.701 : ---5-->    tt4.png | 34.15dB\n",
      "22-01-12 02:37:13.840 : ---6-->    tt5.png | 30.97dB\n",
      "22-01-12 02:37:13.988 : ---7-->    tt6.png | 26.41dB\n",
      "22-01-12 02:37:14.101 : ---8-->    tt7.png | 27.73dB\n",
      "22-01-12 02:37:14.219 : ---9-->    tt8.png | 35.44dB\n",
      "22-01-12 02:37:14.353 : --10-->    tt9.png | 34.55dB\n",
      "22-01-12 02:37:14.415 : <epoch:1470, iter:  25,000, Average PSNR : 32.08dB\n",
      "\n",
      "22-01-12 02:41:35.976 : <epoch:1499, iter:  25,500, lr:2.000e-04> G_loss: 2.842e-02 \n",
      "22-01-12 02:46:01.029 : <epoch:1529, iter:  26,000, lr:2.000e-04> G_loss: 3.172e-02 \n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python main_train_psnr.py --opt options/swinir/train_swinir_sr_classical.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a9d4aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from superresolution/swinir_sr_classical_patch48_x3/models/25000_G.pth\n",
      "Testing 0 00                  \n",
      "Testing 1 01                  \n",
      "Testing 2 02                  \n",
      "Testing 3 03                  \n",
      "Testing 4 04                  \n",
      "Testing 5 05                  \n",
      "Testing 6 06                  \n",
      "Testing 7 07                  \n",
      "Testing 8 08                  \n",
      "Testing 9 09                  \n",
      "Testing 10 10                  \n",
      "Testing 11 11                  \n",
      "Testing 12 12                  \n",
      "Testing 13 13                  \n"
     ]
    }
   ],
   "source": [
    "!python main_test_swinir.py --task classical_sr --scale 3 --training_patch_size 48 --model_path superresolution/swinir_sr_classical_patch48_x3/models/25000_G.pth --folder_lq '/home/wschenst06g/Ming/MMSE_global/dodo/KAIR/testsets/test' --folder_gt testsets/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30def1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
